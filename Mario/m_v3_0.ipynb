{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30776,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yomunozs/Aprendizaje_Maquina/blob/main/Mario/m_v3_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Install virtualenv if you don't have it\n",
        "# !pip install virtualenv\n",
        "\n",
        "# # Create a virtual environment with Python 3.6\n",
        "# !virtualenv -p python3.6 tf_env\n",
        "\n",
        "# # Activate the virtual environment\n",
        "# # On Windows\n",
        "# tf_env\\Scripts\\activate\n",
        "# # On macOS/Linux\n",
        "# source tf_env/bin/activate"
      ],
      "metadata": {
        "id": "6ldDq52Hjsr6",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:46:03.494621Z",
          "iopub.execute_input": "2024-10-01T18:46:03.495165Z",
          "iopub.status.idle": "2024-10-01T18:46:03.499883Z",
          "shell.execute_reply.started": "2024-10-01T18:46:03.495128Z",
          "shell.execute_reply": "2024-10-01T18:46:03.499029Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages with specific versions for compatibility\n",
        "# !pip install gym==0.17.2\n",
        "# !pip install gym-super-mario-bros==7.3.0\n",
        "# !pip install nes-py==8.1.8\n",
        "# !pip install tf-agents==0.5.0"
      ],
      "metadata": {
        "id": "zdpyqoaajsr9",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:46:03.503804Z",
          "iopub.execute_input": "2024-10-01T18:46:03.504096Z",
          "iopub.status.idle": "2024-10-01T18:46:03.513789Z",
          "shell.execute_reply.started": "2024-10-01T18:46:03.504064Z",
          "shell.execute_reply": "2024-10-01T18:46:03.512798Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow==1.15.2"
      ],
      "metadata": {
        "id": "2y4XBpXQjsr-",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:46:03.515532Z",
          "iopub.execute_input": "2024-10-01T18:46:03.515907Z",
          "iopub.status.idle": "2024-10-01T18:46:03.522218Z",
          "shell.execute_reply.started": "2024-10-01T18:46:03.515873Z",
          "shell.execute_reply": "2024-10-01T18:46:03.521277Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "XJx-Dxnpjsr-",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:46:03.523486Z",
          "iopub.execute_input": "2024-10-01T18:46:03.523832Z",
          "iopub.status.idle": "2024-10-01T18:46:03.532831Z",
          "shell.execute_reply.started": "2024-10-01T18:46:03.523791Z",
          "shell.execute_reply": "2024-10-01T18:46:03.531875Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get -q update -q\n",
        "!sudo apt-get install -y -q xvfb ffmpeg freeglut3-dev -q\n",
        "print('DONE1')\n",
        "!pip install -q 'imageio==2.4.0' -q\n",
        "!pip install -q pyvirtualdisplay -q\n",
        "print('DONE2')\n",
        "!pip install -q tf-agents[reverb] -q\n",
        "!pip install -q pyglet -q\n",
        "print('DONE3')\n",
        "!pip install -q swig -q\n",
        "!pip install -q gym[atari,box2d,accept-rom-license] -q #install gym and virtual display\n",
        "print ('DONE4')\n",
        "!pip install -q gym-super-mario-bros -q\n",
        "print('DONE')"
      ],
      "metadata": {
        "id": "Ufji_JwDjsr_",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:46:03.534889Z",
          "iopub.execute_input": "2024-10-01T18:46:03.535174Z",
          "iopub.status.idle": "2024-10-01T18:51:27.796359Z",
          "shell.execute_reply.started": "2024-10-01T18:46:03.535143Z",
          "shell.execute_reply": "2024-10-01T18:51:27.795151Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "import tf_agents\n",
        "from tf_agents.environments import suite_gym, tf_py_environment\n",
        "from tf_agents.environments.wrappers import ActionRepeat\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "\n",
        "\n",
        "from tf_agents.environments import gym_wrapper\n",
        "from tf_agents.environments import tf_py_environment\n",
        "\n",
        "import gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym.wrappers import TimeLimit\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
        "\n",
        "# To get smooth animations\n",
        "import matplotlib.animation as animation\n",
        "matplotlib.rc('animation', html='jshtml')\n",
        "\n",
        "# Print versions of imported packages\n",
        "print(f\"imageio version: {imageio.__version__}\")\n",
        "print(f\"pyvirtualdisplay version: {pyvirtualdisplay.__version__}\")\n",
        "\n",
        "# Print TensorFlow version separately\n",
        "print(f\"tensorflow version: {tf.__version__}\")\n",
        "\n",
        "# Print tf-agents version\n",
        "try:\n",
        "    print(f\"tf_agents version: {tf_agents.__version__}\")\n",
        "except AttributeError:\n",
        "    print(\"tf_gents version: not available\")\n",
        "\n",
        "# Print other versions\n",
        "print(f\"gym version: {gym.__version__}\")\n",
        "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
        "print(f\"PIL version: {PIL.Image.__version__}\")  # Use PIL.Image to get version\n",
        "\n",
        "# Handle reverb\n",
        "try:\n",
        "    import pkg_resources\n",
        "    reverb_version = pkg_resources.get_distribution(\"reverb\").version\n",
        "    print(f\"reverb version: {reverb_version}\")\n",
        "except Exception:\n",
        "    print(\"reverb version: not available\")\n",
        "\n",
        "# Handle gym-super-mario-bros\n",
        "try:\n",
        "    print(f\"gym-super-mario-bros version: {gym_super_mario_bros.__version__}\")\n",
        "except AttributeError:\n",
        "    print(\"gym-super-mario-bros version: not available\")"
      ],
      "metadata": {
        "id": "PmeeQg6yjsr_",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:27.797877Z",
          "iopub.execute_input": "2024-10-01T18:51:27.798205Z",
          "iopub.status.idle": "2024-10-01T18:51:38.214342Z",
          "shell.execute_reply.started": "2024-10-01T18:51:27.798169Z",
          "shell.execute_reply": "2024-10-01T18:51:38.213200Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v2')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "print(SIMPLE_MOVEMENT)\n",
        "print(COMPLEX_MOVEMENT)\n",
        "print(RIGHT_ONLY)\n",
        "state = env.reset()\n",
        "env.step(env._action_space.sample())\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "plt.figure(figsize=(4, 6))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CBPKx5IBjssA",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:38.215836Z",
          "iopub.execute_input": "2024-10-01T18:51:38.216931Z",
          "iopub.status.idle": "2024-10-01T18:51:38.804112Z",
          "shell.execute_reply.started": "2024-10-01T18:51:38.216882Z",
          "shell.execute_reply": "2024-10-01T18:51:38.802822Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear y envolver el entorno\n",
        "def create_environment():\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-v2')# Cargar el entorno de Super Mario Bros pixelado por simplificacion\n",
        "    #env = TimeLimit(env, max_episode_steps=max_steps)\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)# usar grupo de movimientos simple\n",
        "    env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)  # Convertir las observaciones a escala de grises\n",
        "    env = gym.wrappers.ResizeObservation(env, 84)  # Redimensionar las observaciones a 84x84 píxeles\n",
        "    env = gym.wrappers.FrameStack(env, 4)  # Apilar las últimas 4 observaciones\n",
        "    env = gym_wrapper.GymWrapper(env) # Envolver el entorno con un envoltorio personalizado\n",
        "\n",
        "    # Retornar el entorno preparado\n",
        "    return env\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "T5azN2PXjssA",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:38.810314Z",
          "iopub.execute_input": "2024-10-01T18:51:38.813665Z",
          "iopub.status.idle": "2024-10-01T18:51:38.832604Z",
          "shell.execute_reply.started": "2024-10-01T18:51:38.813598Z",
          "shell.execute_reply": "2024-10-01T18:51:38.830186Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create and wrap the environment prueba diferente codigo\n",
        "# max_steps = 100000\n",
        "# def create_environment(max_steps=100000):\n",
        "#     # Load the Super Mario Bros environment\n",
        "#     env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
        "# #     env = TimeLimit(env, max_episode_steps=max_steps)\n",
        "#     env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "#     env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)\n",
        "#     env = gym.wrappers.ResizeObservation(env, 84)\n",
        "#     env = gym.wrappers.FrameStack(env, 4)\n",
        "#     return env\n",
        "# print('Done')"
      ],
      "metadata": {
        "id": "prGPs--tjssB",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:38.838839Z",
          "iopub.execute_input": "2024-10-01T18:51:38.842033Z",
          "iopub.status.idle": "2024-10-01T18:51:39.062124Z",
          "shell.execute_reply.started": "2024-10-01T18:51:38.841981Z",
          "shell.execute_reply": "2024-10-01T18:51:39.061180Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar entornos para entrenamiento y evaluación\n",
        "train_py_env = create_environment()  # Crea el entorno para entrenamiento\n",
        "eval_py_env = create_environment()   # Crea el entorno para evaluación\n",
        "\n",
        "# train_py_env1 = create_environment1()  # Crea un segundo entorno para entrenamiento\n",
        "# eval_py_env1 = create_environment1()   # Crea un segundo entorno para evaluación\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "9R90iWu0jssB",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:39.063734Z",
          "iopub.execute_input": "2024-10-01T18:51:39.064040Z",
          "iopub.status.idle": "2024-10-01T18:51:39.907993Z",
          "shell.execute_reply.started": "2024-10-01T18:51:39.064007Z",
          "shell.execute_reply": "2024-10-01T18:51:39.907039Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Límite de pasos por episodio:\", eval_py_env.spec.max_episode_steps)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:39.911381Z",
          "iopub.execute_input": "2024-10-01T18:51:39.911718Z",
          "iopub.status.idle": "2024-10-01T18:51:39.917218Z",
          "shell.execute_reply.started": "2024-10-01T18:51:39.911678Z",
          "shell.execute_reply": "2024-10-01T18:51:39.916140Z"
        },
        "trusted": true,
        "id": "FGqts93jPJni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Agents environments\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)  # Convertir el entorno de entrenamiento a un entorno de TensorFlow Agents\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)    # Convertir el entorno de evaluación a un entorno de TensorFlow Agents\n",
        "\n",
        "\n",
        "#train_env1 = tf_py_environment.TFPyEnvironment(train_py_env1)\n",
        "#eval_env1 = tf_py_environment.TFPyEnvironment(eval_py_env1)\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "HWSV_D1jjssC",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:39.918288Z",
          "iopub.execute_input": "2024-10-01T18:51:39.918652Z",
          "iopub.status.idle": "2024-10-01T18:51:39.937160Z",
          "shell.execute_reply.started": "2024-10-01T18:51:39.918611Z",
          "shell.execute_reply": "2024-10-01T18:51:39.936181Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la red Q\n",
        "preprocessing_layer = tf.keras.layers.Lambda(lambda x: tf.cast(x, np.float32) / 255.)  # Se crea una capa de preprocesamiento para escalar las imágenes entre 0 y 1\n",
        "conv_layer_params = [  # Se definen los parámetros para las capas convolucionales\n",
        "    (32, (8, 8), 4),  # Se configura la primera capa convolucional (número de filtros, tamaño del kernel, paso)\n",
        "    (64, (4, 4), 2),  # Se configura la segunda capa convolucional\n",
        "    (64, (3, 3), 1),  # Se configura la tercera capa convolucional\n",
        "]\n",
        "\n",
        "fc_layer_params = [512]  # Se definen los parámetros para las capas totalmente conectadas 'neuronas'\n",
        "\n",
        "# Crear la red Q\n",
        "q_net = QNetwork(\n",
        "    input_tensor_spec = train_env.observation_spec(),  # Se especifica la entrada del entorno de entrenamiento\n",
        "    action_spec = train_env.action_spec(),  # Se especifican las acciones del entorno de entrenamiento\n",
        "    preprocessing_layers = preprocessing_layer,  # Se asigna la capa de preprocesamiento\n",
        "    conv_layer_params = conv_layer_params,  # Se asignan los parámetros de las capas convolucionales\n",
        "    fc_layer_params = fc_layer_params  # Se asignan los parámetros de las capas totalmente conectadas\n",
        ")\n",
        "\n",
        "# Crear el agente DQN\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(  # Se define el optimizador RMSProp\n",
        "    learning_rate = 2.5e-4,  # Se establece la tasa de aprendizaje\n",
        "    decay = 0.95,  # Se configura el decaimiento del optimizador\n",
        "    momentum = 0.0,  # Se establece el momentum del optimizador\n",
        "    epsilon = 0.01,  # Se configura el epsilon para el optimizador\n",
        "    centered = True  # Se utiliza la versión centrada del RMSProp\n",
        ")\n",
        "\n",
        "train_step_counter = tf.Variable(0)  # Se inicializa un contador para los pasos de entrenamiento\n",
        "\n",
        "# Inicializar el agente DQN\n",
        "agent = DqnAgent(\n",
        "    time_step_spec = train_env.time_step_spec(),  # Se especifica el tiempo del entorno de entrenamiento\n",
        "    action_spec = train_env.action_spec(),  # Se especifican las acciones del entorno de entrenamiento\n",
        "    q_network = q_net,  # Se asigna la red Q que se utilizará\n",
        "    optimizer = optimizer,  # Se asigna el optimizador para el agente\n",
        "    td_errors_loss_fn = common.element_wise_squared_loss,  # Se define la función de pérdida para errores temporales\n",
        "    train_step_counter = train_step_counter,  # Se asigna el contador de pasos de entrenamiento\n",
        "    gamma = 0.99,  # Se establece el factor de descuento\n",
        "    epsilon_greedy = 0.1,  # Se configura la probabilidad de elegir una acción aleatoria\n",
        "    target_update_period = 10000  # Se establece la frecuencia para actualizar el objetivo\n",
        ")\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "T2mkoaqRjssC",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:39.938526Z",
          "iopub.execute_input": "2024-10-01T18:51:39.938867Z",
          "iopub.status.idle": "2024-10-01T18:51:42.953496Z",
          "shell.execute_reply.started": "2024-10-01T18:51:39.938831Z",
          "shell.execute_reply": "2024-10-01T18:51:42.952471Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.initialize()  # Se inicializa el agente DQN\n",
        "\n",
        "# Create the replay buffer\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec = agent.collect_data_spec,  # Se especifica la estructura de los datos que se recogerán\n",
        "    batch_size = train_env.batch_size,  # Se establece el tamaño del lote para el buffer, acciones, recompensas y otros elementos relevantes.\n",
        "    max_length = 100000  # Se define la longitud máxima del buffer de repetición, se actualiza\n",
        ")\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "hpklgOwmjssC",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:42.954770Z",
          "iopub.execute_input": "2024-10-01T18:51:42.955159Z",
          "iopub.status.idle": "2024-10-01T18:51:43.721814Z",
          "shell.execute_reply.started": "2024-10-01T18:51:42.955124Z",
          "shell.execute_reply": "2024-10-01T18:51:43.720832Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para recolectar experiencia\n",
        "def collect_step(environment, policy, buffer):\n",
        "    time_step = environment.current_time_step()  # Se obtiene el estado actual del entorno\n",
        "    action_step = policy.action(time_step)  # Se calcula la acción a partir del estado actual\n",
        "    next_time_step = environment.step(action_step.action)  # Se aplica la acción en el entorno y se obtiene el siguiente estado\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)  # Se crea una trayectoria a partir de la transición\n",
        "    buffer.add_batch(traj)  # Se añade la trayectoria al buffer de experiencias\n",
        "\n",
        "# Se recolectan datos iniciales con una política aleatoria\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())  # Se define una política aleatoria\n",
        "initial_collect_steps = 1000  # Se establece el número de pasos iniciales de recolección\n",
        "for _ in range(initial_collect_steps):\n",
        "    collect_step(train_env, random_policy, replay_buffer)  # Se recolecta experiencia utilizando la política aleatoria\n",
        "\n",
        "# Se prepara el conjunto de datos\n",
        "dataset = replay_buffer.as_dataset(  # Se convierte el buffer en un conjunto de datos\n",
        "    num_parallel_calls=3,  # Se establece el número de llamadas paralelas\n",
        "    sample_batch_size=32,  # Se define el tamaño del lote de muestra\n",
        "    num_steps=2  # Se especifica el número de pasos a considerar en cada muestra\n",
        ").prefetch(3)  # Se pre-carga el conjunto de datos\n",
        "iterator = iter(dataset)  # Se crea un iterador para el conjunto de datos\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "M-60JcY-jssD",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:51:43.723091Z",
          "iopub.execute_input": "2024-10-01T18:51:43.723499Z",
          "iopub.status.idle": "2024-10-01T18:51:56.818918Z",
          "shell.execute_reply.started": "2024-10-01T18:51:43.723427Z",
          "shell.execute_reply": "2024-10-01T18:51:56.817888Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del agente\n",
        "num_iterations = 200000  # Se ajusta este valor según los límites computacionales de Kaggle\n",
        "collect_steps_per_iteration = 1  # Se define el número de pasos de recolección por iteración\n",
        "log_interval = 1000  # Se establece el intervalo para los registros\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    # Recolectar experiencia\n",
        "    for _ in range(collect_steps_per_iteration):\n",
        "        collect_step(train_env, agent.collect_policy, replay_buffer)  # Se llama a la función para recolectar experiencias\n",
        "\n",
        "    # Muestra una experiencia del buffer y entrena al agente\n",
        "    experience, _ = next(iterator)  # Se extrae un lote de experiencia del buffer\n",
        "    train_loss = agent.train(experience).loss  # Se entrena al agente y se obtiene la pérdida\n",
        "\n",
        "\n",
        "\n",
        "    if iteration % log_interval == 0:  # para cada intervalo\n",
        "        print(f'Iteración: {iteration}, Pérdida: {train_loss}')  # Se imprime la iteración y la pérdida\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B8znQf-yjssD",
        "execution": {
          "iopub.status.busy": "2024-10-01T18:53:07.562231Z",
          "iopub.execute_input": "2024-10-01T18:53:07.563261Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Evaluar el rendimiento del agente\n",
        "# log_interval = 500  # Se define el intervalo para registrar información\n",
        "# num_eval_episodes = 5  # Se establece el número de episodios de evaluación\n",
        "\n",
        "# for episode in range(num_eval_episodes):  # Se itera sobre el número de episodios de evaluación\n",
        "#     time_step = eval_env.reset()  # Se reinicia el entorno de evaluación\n",
        "#     episode_reward = 0  # Se inicializa la recompensa total del episodio\n",
        "#     i = 0  # Se inicializa un contador de iteraciones\n",
        "\n",
        "#     while not time_step.is_last():  # Se ejecuta mientras no se alcance el final del episodio\n",
        "#         action_step = agent.policy.action(time_step)  # Se selecciona una acción según la política del agente\n",
        "#         time_step = eval_env.step(action_step.action)  # Se avanza en el entorno con la acción seleccionada\n",
        "#         episode_reward += time_step.reward  # Se acumula la recompensa del paso actual\n",
        "#         i += 1  # Se incrementa el contador de iteraciones\n",
        "\n",
        "#         if i % log_interval == 0:  # Se verifica si se alcanza el intervalo de registro\n",
        "#             print(f'Episode {episode + 1}: Ite.: {i} :episode_reward : {episode_reward.numpy()[0]}')  # Se imprime la recompensa acumulada\n"
      ],
      "metadata": {
        "id": "CRkMNR-ojssD",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar una pantalla virtual para renderizar entornos de OpenAI Gym\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()  # Se inicia la pantalla virtual\n",
        "\n",
        "# Funciones de visualización proporcionadas\n",
        "def update_scene(num, frames, patch):  # Se define la función para actualizar la escena\n",
        "    patch.set_data(frames[num])  # Se actualizan los datos del frame actual\n",
        "    return patch  # Se devuelve el parche actualizado\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):  # Se define la función para crear la animación\n",
        "    fig = plt.figure()  # Se crea una nueva figura\n",
        "    patch = plt.imshow(frames[0])  # Se muestra el primer frame\n",
        "    plt.axis('off')  # Se ocultan los ejes\n",
        "    anim = animation.FuncAnimation(  # Se crea la animación\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()  # Se cierra la figura para no mostrarla dos veces\n",
        "    return anim  # Se devuelve la animación creada\n",
        "\n",
        "# Recoger frames para la visualización\n",
        "def run_and_visualize(agent, env):\n",
        "    frames = []\n",
        "    time_step = env.reset()\n",
        "    policy_state = agent.policy.get_initial_state(env.batch_size)\n",
        "    j = 0\n",
        "    while not time_step.is_last():\n",
        "    #for j in range(10000):\n",
        "        action_step = agent.policy.action(time_step, policy_state)\n",
        "        policy_state = action_step.state\n",
        "        time_step = env.step(action_step.action)\n",
        "        frame = np.squeeze(env.render())\n",
        "        frames.append(frame)\n",
        "\n",
        "        if j % 2000 == 0:\n",
        "\n",
        "\n",
        "            print(f\"Step: {len(frames)}, Action: {action_step.action}, Frame shape: {frame.shape}\")\n",
        "        j += 1\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Total frames collected: {len(frames)}\")\n",
        "    return frames\n",
        "\n",
        "# Ejecutar el agente y recoger marcos\n",
        "frames = run_and_visualize(agent, eval_env)  # Se ejecuta la función para recoger marcos\n",
        "\n",
        "# Crear y mostrar la animación\n",
        "anim = plot_animation(frames)  # Se crea la animación a partir de los marcos\n",
        "\n",
        "# Mostrar la animación en Jupyter Notebook\n",
        "from IPython.display import HTML  # Se importa la librería necesaria\n",
        "HTML(anim.to_jshtml())  # Se convierte la animación a formato HTML y se muestra\n"
      ],
      "metadata": {
        "id": "hhhZoCW9jssE",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "id": "baEJdFjgjssE",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mario aleatorio****"
      ],
      "metadata": {
        "id": "wDF1SPhnjssE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v2')\n",
        "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "print(env.get_action_meanings())\n",
        "state = env.reset()\n",
        "env.step(env._action_space.sample())\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "plt.figure(figsize=(4, 6))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Create and wrap the environment\n",
        "def create_environment1():\n",
        "    # Load the Super Mario Bros environment\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-v1')\n",
        "    # Simplify the action space\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "    # Apply preprocessing wrappers\n",
        "    env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)\n",
        "    env = gym.wrappers.ResizeObservation(env, 84)\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "    env = gym_wrapper.GymWrapper(env)\n",
        "    return env\n",
        "\n",
        "eval_py_env1 = create_environment1()\n",
        "\n",
        "eval_env1 = tf_py_environment.TFPyEnvironment(eval_py_env1)\n",
        "# Configurar una pantalla virtual para renderizar entornos de OpenAI Gym\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()  # Se inicia la pantalla virtual\n",
        "\n",
        "# Funciones de visualización proporcionadas\n",
        "def update_scene(num, frames, patch):  # Se define la función para actualizar la escena\n",
        "    patch.set_data(frames[num])  # Se actualizan los datos del frame actual\n",
        "    return patch  # Se devuelve el parche actualizado\n",
        "\n",
        "def plot_animation1(frames, repeat=False, interval=40):  # Se define la función para crear la animación\n",
        "    fig = plt.figure()  # Se crea una nueva figura\n",
        "    patch = plt.imshow(frames[0])  # Se muestra el primer frame\n",
        "    plt.axis('off')  # Se ocultan los ejes\n",
        "    anim = animation.FuncAnimation(  # Se crea la animación\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()  # Se cierra la figura para no mostrarla dos veces\n",
        "    return anim  # Se devuelve la animación creada\n",
        "def random_policy():\n",
        "    return np.random.choice(len(SIMPLE_MOVEMENT))\n",
        "# Recoger frames para la visualización\n",
        "def run_and_visualize1(env):\n",
        "    frames = []\n",
        "    time_step = env.reset()\n",
        "    #policy_state = agent.policy.get_initial_state(env.batch_size)\n",
        "\n",
        "    for i in range(8000):\n",
        "        action_step = random_policy()\n",
        "        #policy_state = action_step.state\n",
        "        time_step = env.step(action_step)\n",
        "\n",
        "        # Obtener el frame y asegurarse de que es un array numpy\n",
        "        frame = np.squeeze(env.render())\n",
        "\n",
        "        frames.append(frame)\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Total frames collected: {len(frames)}\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "# Ejecutar el agente y recoger marcos\n",
        "frames_random = run_and_visualize1(eval_env1)  # Se ejecuta la función para recoger marcos\n",
        "\n",
        "# Crear y mostrar la animación\n",
        "anim = plot_animation1(frames_random)  # Se crea la animación a partir de los marcos\n",
        "\n",
        "# Mostrar la animación en Jupyter Notebook\n",
        "from IPython.display import HTML  # Se importa la librería necesaria\n",
        "HTML(anim.to_jshtml())  # Se convierte la animación a formato HTML y se muestra\n"
      ],
      "metadata": {
        "id": "yQwJQzc_jssF",
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YJbVcAq9jssG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}