{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30776,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yomunozs/Aprendizaje_Maquina/blob/main/Mario/smb_p1_v2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Install virtualenv if you don't have it\n",
        "# !pip install virtualenv\n",
        "\n",
        "# # Create a virtual environment with Python 3.6\n",
        "# !virtualenv -p python3.6 tf_env\n",
        "\n",
        "# # Activate the virtual environment\n",
        "# # On Windows\n",
        "# tf_env\\Scripts\\activate\n",
        "# # On macOS/Linux\n",
        "# source tf_env/bin/activate"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T20:35:53.328488Z",
          "iopub.execute_input": "2024-09-29T20:35:53.328862Z",
          "iopub.status.idle": "2024-09-29T20:35:53.333309Z",
          "shell.execute_reply.started": "2024-09-29T20:35:53.328826Z",
          "shell.execute_reply": "2024-09-29T20:35:53.332306Z"
        },
        "trusted": true,
        "id": "84dQKs_hOXlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages with specific versions for compatibility\n",
        "# !pip install gym==0.17.2\n",
        "# !pip install gym-super-mario-bros==7.3.0\n",
        "# !pip install nes-py==8.1.8\n",
        "# !pip install tf-agents==0.5.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T20:35:53.431963Z",
          "iopub.execute_input": "2024-09-29T20:35:53.432338Z",
          "iopub.status.idle": "2024-09-29T20:35:53.436901Z",
          "shell.execute_reply.started": "2024-09-29T20:35:53.432301Z",
          "shell.execute_reply": "2024-09-29T20:35:53.435838Z"
        },
        "trusted": true,
        "id": "xODYmLGtOXlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow==1.15.2"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T20:35:53.438653Z",
          "iopub.execute_input": "2024-09-29T20:35:53.439012Z",
          "iopub.status.idle": "2024-09-29T20:35:53.444339Z",
          "shell.execute_reply.started": "2024-09-29T20:35:53.438967Z",
          "shell.execute_reply": "2024-09-29T20:35:53.443430Z"
        },
        "trusted": true,
        "id": "d_3gAHRxOXlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T20:35:53.539290Z",
          "iopub.execute_input": "2024-09-29T20:35:53.539584Z",
          "iopub.status.idle": "2024-09-29T20:35:53.543577Z",
          "shell.execute_reply.started": "2024-09-29T20:35:53.539553Z",
          "shell.execute_reply": "2024-09-29T20:35:53.542547Z"
        },
        "trusted": true,
        "id": "N_A5TaUqOXlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get -q update -q\n",
        "!sudo apt-get install -y -q xvfb ffmpeg freeglut3-dev -q\n",
        "print('DONE1')\n",
        "!pip install -q 'imageio==2.4.0' -q\n",
        "!pip install -q pyvirtualdisplay -q\n",
        "print('DONE2')\n",
        "!pip install -q tf-agents[reverb] -q\n",
        "!pip install -q pyglet -q\n",
        "print('DONE3')\n",
        "!pip install -q swig -q\n",
        "!pip install -q gym[atari,box2d,accept-rom-license] -q #install gym and virtual display\n",
        "print ('DONE4')\n",
        "!pip install -q gym-super-mario-bros -q\n",
        "print('DONE')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T17:47:10.791101Z",
          "iopub.execute_input": "2024-10-01T17:47:10.791473Z",
          "iopub.status.idle": "2024-10-01T17:51:41.850802Z",
          "shell.execute_reply.started": "2024-10-01T17:47:10.791436Z",
          "shell.execute_reply": "2024-10-01T17:51:41.849543Z"
        },
        "trusted": true,
        "id": "MIB9eT6dOXlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "import tf_agents\n",
        "from tf_agents.environments import suite_gym, tf_py_environment\n",
        "from tf_agents.environments.wrappers import ActionRepeat\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.replay_buffers import TFUniformReplayBuffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
        "from tf_agents.policies import epsilon_greedy_policy\n",
        "\n",
        "from tf_agents.environments import gym_wrapper\n",
        "from tf_agents.environments import tf_py_environment\n",
        "\n",
        "import gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\n",
        "\n",
        "# To get smooth animations\n",
        "import matplotlib.animation as animation\n",
        "matplotlib.rc('animation', html='jshtml')\n",
        "\n",
        "# Print versions of imported packages\n",
        "print(f\"imageio version: {imageio.__version__}\")\n",
        "print(f\"pyvirtualdisplay version: {pyvirtualdisplay.__version__}\")\n",
        "\n",
        "# Print TensorFlow version separately\n",
        "print(f\"tensorflow version: {tf.__version__}\")\n",
        "\n",
        "# Print tf-agents version\n",
        "try:\n",
        "    print(f\"tf_agents version: {tf_agents.__version__}\")\n",
        "except AttributeError:\n",
        "    print(\"tf_gents version: not available\")\n",
        "\n",
        "# Print other versions\n",
        "print(f\"gym version: {gym.__version__}\")\n",
        "print(f\"matplotlib version: {matplotlib.__version__}\")\n",
        "print(f\"PIL version: {PIL.Image.__version__}\")  # Use PIL.Image to get version\n",
        "\n",
        "# Handle reverb\n",
        "try:\n",
        "    import pkg_resources\n",
        "    reverb_version = pkg_resources.get_distribution(\"reverb\").version\n",
        "    print(f\"reverb version: {reverb_version}\")\n",
        "except Exception:\n",
        "    print(\"reverb version: not available\")\n",
        "\n",
        "# Handle gym-super-mario-bros\n",
        "try:\n",
        "    print(f\"gym-super-mario-bros version: {gym_super_mario_bros.__version__}\")\n",
        "except AttributeError:\n",
        "    print(\"gym-super-mario-bros version: not available\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T17:51:41.853157Z",
          "iopub.execute_input": "2024-10-01T17:51:41.853478Z",
          "iopub.status.idle": "2024-10-01T17:51:49.784765Z",
          "shell.execute_reply.started": "2024-10-01T17:51:41.853442Z",
          "shell.execute_reply": "2024-10-01T17:51:49.783875Z"
        },
        "trusted": true,
        "id": "ICROCQrgOXlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym_super_mario_bros.make('SuperMarioBros-v2')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "print(SIMPLE_MOVEMENT)\n",
        "print(COMPLEX_MOVEMENT)\n",
        "print(RIGHT_ONLY)\n",
        "state = env.reset()\n",
        "env.step(env._action_space.sample())\n",
        "img = env.render(mode=\"rgb_array\")\n",
        "plt.figure(figsize=(4, 6))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:37:11.310909Z",
          "iopub.execute_input": "2024-10-01T18:37:11.311366Z",
          "iopub.status.idle": "2024-10-01T18:37:11.785195Z",
          "shell.execute_reply.started": "2024-10-01T18:37:11.311323Z",
          "shell.execute_reply": "2024-10-01T18:37:11.784244Z"
        },
        "trusted": true,
        "id": "rj1m3aYVOXlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear y envolver el entorno\n",
        "def create_environment():\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-v2')# Cargar el entorno de Super Mario Bros pixelado por simplificacion\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)# usar grupo de movimientos simple\n",
        "    env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)  # Convertir las observaciones a escala de grises\n",
        "    env = gym.wrappers.ResizeObservation(env, 84)  # Redimensionar las observaciones a 84x84 píxeles\n",
        "    env = gym.wrappers.FrameStack(env, 4)  # Apilar las últimas 4 observaciones\n",
        "    env = gym_wrapper.GymWrapper(env) # Envolver el entorno con un envoltorio personalizado\n",
        "\n",
        "    # Retornar el entorno preparado\n",
        "    return env\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:37:13.443981Z",
          "iopub.execute_input": "2024-10-01T18:37:13.444412Z",
          "iopub.status.idle": "2024-10-01T18:37:13.451294Z",
          "shell.execute_reply.started": "2024-10-01T18:37:13.444371Z",
          "shell.execute_reply": "2024-10-01T18:37:13.450360Z"
        },
        "trusted": true,
        "id": "mzSBPkLfOXlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Max episode steps: {env.spec.max_episode_steps}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:37:15.657233Z",
          "iopub.execute_input": "2024-10-01T18:37:15.657612Z",
          "iopub.status.idle": "2024-10-01T18:37:15.662762Z",
          "shell.execute_reply.started": "2024-10-01T18:37:15.657575Z",
          "shell.execute_reply": "2024-10-01T18:37:15.661777Z"
        },
        "trusted": true,
        "id": "BkMYHftNOXlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and wrap the environment prueba diferente codigo\n",
        "def create_environment1():\n",
        "    # Load the Super Mario Bros environment\n",
        "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v2')\n",
        "    env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
        "    env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)\n",
        "    env = gym.wrappers.ResizeObservation(env, 84)\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "    env = gym_wrapper.GymWrapper(env) # Envolver el entorno con un envoltorio personalizado\n",
        "    return env\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:37:17.721529Z",
          "iopub.execute_input": "2024-10-01T18:37:17.721917Z",
          "iopub.status.idle": "2024-10-01T18:37:17.728314Z",
          "shell.execute_reply.started": "2024-10-01T18:37:17.721878Z",
          "shell.execute_reply": "2024-10-01T18:37:17.727352Z"
        },
        "trusted": true,
        "id": "YafCq_JTOXlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciar entornos para entrenamiento y evaluación\n",
        "train_py_env = create_environment()  # Crea el entorno para entrenamiento\n",
        "eval_py_env = create_environment()   # Crea el entorno para evaluación\n",
        "\n",
        "train_py_env1 = create_environment1()  # Crea un segundo entorno para entrenamiento\n",
        "eval_py_env1 = create_environment1()   # Crea un segundo entorno para evaluación\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:37:19.298333Z",
          "iopub.execute_input": "2024-10-01T18:37:19.299013Z",
          "iopub.status.idle": "2024-10-01T18:37:20.931167Z",
          "shell.execute_reply.started": "2024-10-01T18:37:19.298971Z",
          "shell.execute_reply": "2024-10-01T18:37:20.930150Z"
        },
        "trusted": true,
        "id": "cPRdVBlMOXlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_env1 = tf_py_environment.TFPyEnvironment(train_py_env1)\n",
        "eval_env1 = tf_py_environment.TFPyEnvironment(eval_py_env1)\n",
        "\n",
        "# Configurar una pantalla virtual para renderizar entornos de OpenAI Gym\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()  # Se inicia la pantalla virtual\n",
        "\n",
        "# Funciones de visualización proporcionadas\n",
        "def update_scene(num, frames, patch):  # Se define la función para actualizar la escena\n",
        "    patch.set_data(frames[num])  # Se actualizan los datos del frame actual\n",
        "    return patch  # Se devuelve el parche actualizado\n",
        "\n",
        "def plot_animation1(frames, repeat=False, interval=40):  # Se define la función para crear la animación\n",
        "    fig = plt.figure()  # Se crea una nueva figura\n",
        "    patch = plt.imshow(frames[0])  # Se muestra el primer frame\n",
        "    plt.axis('off')  # Se ocultan los ejes\n",
        "    anim = animation.FuncAnimation(  # Se crea la animación\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()  # Se cierra la figura para no mostrarla dos veces\n",
        "    return anim  # Se devuelve la animación creada\n",
        "def random_policy():\n",
        "    return np.random.choice(len(SIMPLE_MOVEMENT))\n",
        "# Recoger frames para la visualización\n",
        "def run_and_visualize1(env):\n",
        "    frames = []\n",
        "    time_step = env.reset()\n",
        "    #policy_state = agent.policy.get_initial_state(env.batch_size)\n",
        "\n",
        "    for i in range(20000):\n",
        "        action_step = random_policy()\n",
        "        #policy_state = action_step.state\n",
        "        time_step = env.step(action_step)\n",
        "\n",
        "        # Obtener el frame y asegurarse de que es un array numpy\n",
        "        frame = np.squeeze(env.render())\n",
        "\n",
        "        frames.append(frame)\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Total frames collected: {len(frames)}\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "# Ejecutar el agente y recoger marcos\n",
        "frames_random = run_and_visualize1(eval_env1)  # Se ejecuta la función para recoger marcos\n",
        "\n",
        "# Crear y mostrar la animación\n",
        "anim = plot_animation1(frames_random)  # Se crea la animación a partir de los marcos\n",
        "\n",
        "# Mostrar la animación en Jupyter Notebook\n",
        "from IPython.display import HTML  # Se importa la librería necesaria\n",
        "HTML(anim.to_jshtml())  # Se convierte la animación a formato HTML y se muestra\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:27:47.344311Z",
          "iopub.execute_input": "2024-10-01T18:27:47.344737Z",
          "iopub.status.idle": "2024-10-01T18:35:52.801649Z",
          "shell.execute_reply.started": "2024-10-01T18:27:47.344699Z",
          "shell.execute_reply": "2024-10-01T18:35:52.799799Z"
        },
        "trusted": true,
        "id": "9-2DgSS4OXlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Agents environments\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)  # Convertir el entorno de entrenamiento a un entorno de TensorFlow Agents\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)    # Convertir el entorno de evaluación a un entorno de TensorFlow Agents\n",
        "\n",
        "\n",
        "#train_env1 = tf_py_environment.TFPyEnvironment(train_py_env1)\n",
        "#eval_env1 = tf_py_environment.TFPyEnvironment(eval_py_env1)\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:37:25.408782Z",
          "iopub.execute_input": "2024-10-01T18:37:25.409210Z",
          "iopub.status.idle": "2024-10-01T18:37:25.421503Z",
          "shell.execute_reply.started": "2024-10-01T18:37:25.409173Z",
          "shell.execute_reply": "2024-10-01T18:37:25.420382Z"
        },
        "trusted": true,
        "id": "0MQScAFmOXlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-EUYCuP3OXlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la red Q\n",
        "preprocessing_layer = tf.keras.layers.Lambda(lambda x: tf.cast(x, np.float32) / 255.)  # Se crea una capa de preprocesamiento para escalar las imágenes entre 0 y 1\n",
        "conv_layer_params = [  # Se definen los parámetros para las capas convolucionales\n",
        "    (32, (8, 8), 4),  # Se configura la primera capa convolucional (número de filtros, tamaño del kernel, paso)\n",
        "    (64, (4, 4), 2),  # Se configura la segunda capa convolucional\n",
        "    (64, (3, 3), 1),  # Se configura la tercera capa convolucional\n",
        "]\n",
        "\n",
        "fc_layer_params = [512]  # Se definen los parámetros para las capas totalmente conectadas 'neuronas'\n",
        "\n",
        "# Crear la red Q\n",
        "q_net = QNetwork(\n",
        "    input_tensor_spec = train_env.observation_spec(),  # Se especifica la entrada del entorno de entrenamiento\n",
        "    action_spec = train_env.action_spec(),  # Se especifican las acciones del entorno de entrenamiento\n",
        "    preprocessing_layers = preprocessing_layer,  # Se asigna la capa de preprocesamiento\n",
        "    conv_layer_params = conv_layer_params,  # Se asignan los parámetros de las capas convolucionales\n",
        "    fc_layer_params = fc_layer_params  # Se asignan los parámetros de las capas totalmente conectadas\n",
        ")\n",
        "\n",
        "\n",
        "# Definir el optimizador\n",
        "optimizer = tf.compat.v1.train.RMSPropOptimizer(  # Se define el optimizador RMSProp\n",
        "    learning_rate = 2.5e-4,  # Se establece la tasa de aprendizaje\n",
        "    decay = 0.95,  # Se configura el decaimiento del optimizador\n",
        "    momentum = 0.0,  # Se establece el momentum del optimizador\n",
        "    epsilon = 0.01,  # Se configura el epsilon para el optimizador\n",
        "    centered = True  # Se utiliza la versión centrada del RMSProp\n",
        ")\n",
        "# Definir el contador global de pasos\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "epsilon = tf.compat.v1.train.polynomial_decay(\n",
        "    learning_rate=1.0,  # Valor inicial de epsilon\n",
        "    global_step=train_step_counter,\n",
        "    decay_steps=10000,  # Número de pasos para reducir epsilon\n",
        "    end_learning_rate=0.01,  # Valor mínimo de epsilon\n",
        "    power=1.0)  # Controla la tasa de decay (1.0 es lineal)\n",
        "\n",
        "# Usar el decay en el agente\n",
        "# Inicializar el agente DQN\n",
        "agent = DqnAgent(\n",
        "    time_step_spec = train_env.time_step_spec(),  # Se especifica el tiempo del entorno de entrenamiento\n",
        "    action_spec = train_env.action_spec(),  # Se especifican las acciones del entorno de entrenamiento\n",
        "    q_network = q_net,  # Se asigna la red Q que se utilizará\n",
        "    optimizer = optimizer,  # Se asigna el optimizador para el agente\n",
        "    td_errors_loss_fn = common.element_wise_squared_loss,  # Se define la función de pérdida para errores temporales\n",
        "    train_step_counter = train_step_counter,  # Se asigna el contador de pasos de entrenamiento\n",
        "    gamma = 0.99,  # Se establece el factor de descuento\n",
        "    epsilon_greedy = epsilon,  # Se configura la probabilidad de elegir una acción aleatoria\n",
        "    target_update_period = 10000  # Se establece la frecuencia para actualizar el objetivo\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:37:27.785017Z",
          "iopub.execute_input": "2024-10-01T18:37:27.785409Z",
          "iopub.status.idle": "2024-10-01T18:37:29.210863Z",
          "shell.execute_reply.started": "2024-10-01T18:37:27.785371Z",
          "shell.execute_reply": "2024-10-01T18:37:29.209865Z"
        },
        "trusted": true,
        "id": "I38eV5YcOXlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent._time_step_spec"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:38:05.533971Z",
          "iopub.execute_input": "2024-10-01T18:38:05.534373Z",
          "iopub.status.idle": "2024-10-01T18:38:05.541983Z",
          "shell.execute_reply.started": "2024-10-01T18:38:05.534336Z",
          "shell.execute_reply": "2024-10-01T18:38:05.540967Z"
        },
        "trusted": true,
        "id": "6azZiln6OXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.initialize()  # Se inicializa el agente DQN\n",
        "\n",
        "# Create the replay buffer\n",
        "replay_buffer = TFUniformReplayBuffer(\n",
        "    data_spec = agent.collect_data_spec,  # Se especifica la estructura de los datos que se recogerán\n",
        "    batch_size = train_env.batch_size,  # Se establece el tamaño del lote para el buffer, acciones, recompensas y otros elementos relevantes.\n",
        "    max_length = 50000  # Se define la longitud máxima del buffer de repetición, se actualiza\n",
        ")\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:38:29.486732Z",
          "iopub.execute_input": "2024-10-01T18:38:29.487137Z",
          "iopub.status.idle": "2024-10-01T18:38:29.630605Z",
          "shell.execute_reply.started": "2024-10-01T18:38:29.487090Z",
          "shell.execute_reply": "2024-10-01T18:38:29.629735Z"
        },
        "trusted": true,
        "id": "999xArqvOXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para recolectar experiencia\n",
        "def collect_step(environment, policy, buffer):\n",
        "    time_step = environment.current_time_step()  # Se obtiene el estado actual del entorno\n",
        "    action_step = policy.action(time_step)  # Se calcula la acción a partir del estado actual\n",
        "    next_time_step = environment.step(action_step.action)  # Se aplica la acción en el entorno y se obtiene el siguiente estado\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)  # Se crea una trayectoria a partir de la transición\n",
        "    buffer.add_batch(traj)  # Se añade la trayectoria al buffer de experiencias\n",
        "\n",
        "# Se recolectan datos iniciales con una política aleatoria\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())  # Se define una política aleatoria\n",
        "initial_collect_steps = 1000  # Se establece el número de pasos iniciales de recolección\n",
        "for _ in range(initial_collect_steps):\n",
        "    collect_step(train_env, random_policy, replay_buffer)  # Se recolecta experiencia utilizando la política aleatoria\n",
        "\n",
        "# Se prepara el conjunto de datos\n",
        "dataset = replay_buffer.as_dataset(  # Se convierte el buffer en un conjunto de datos\n",
        "    num_parallel_calls=3,  # Se establece el número de llamadas paralelas\n",
        "    sample_batch_size=64,  # Se define el tamaño del lote de muestra\n",
        "    num_steps=2  # Se especifica el número de pasos a considerar en cada muestra\n",
        ").prefetch(3)  # Se pre-carga el conjunto de datos\n",
        "iterator = iter(dataset)  # Se crea un iterador para el conjunto de datos\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:38:29.632698Z",
          "iopub.execute_input": "2024-10-01T18:38:29.633020Z",
          "iopub.status.idle": "2024-10-01T18:38:42.592678Z",
          "shell.execute_reply.started": "2024-10-01T18:38:29.632986Z",
          "shell.execute_reply": "2024-10-01T18:38:42.591679Z"
        },
        "trusted": true,
        "id": "2Q711hcPOXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(agent, eval_env, num_episodes=1):\n",
        "    total_reward = 0.0\n",
        "    for episode in range(num_episodes):\n",
        "        time_step = eval_env.reset()\n",
        "        policy_state = agent.policy.get_initial_state(eval_env.batch_size)\n",
        "        episode_reward = 0\n",
        "\n",
        "        while not time_step.is_last():\n",
        "            action_step = agent.policy.action(time_step, policy_state)\n",
        "            time_step = eval_env.step(action_step.action)\n",
        "            episode_reward += time_step.reward\n",
        "\n",
        "        total_reward += episode_reward\n",
        "    avg_reward = total_reward / num_episodes\n",
        "    return avg_reward\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:38:42.594115Z",
          "iopub.execute_input": "2024-10-01T18:38:42.594884Z",
          "iopub.status.idle": "2024-10-01T18:38:42.602019Z",
          "shell.execute_reply.started": "2024-10-01T18:38:42.594814Z",
          "shell.execute_reply": "2024-10-01T18:38:42.601199Z"
        },
        "trusted": true,
        "id": "2cl60Ub9OXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamiento del agente\n",
        "num_iterations = 200000  # Se ajusta este valor según los límites computacionales de Kaggle\n",
        "collect_steps_per_iteration = 1  # Se define el número de pasos de recolección por iteración\n",
        "log_interval = 500  # Se establece el intervalo para los registros\n",
        "eval_interval = 30000  # Evaluar el agente cada 1000 iteraciones\n",
        "checkpoint_dir = 'checkpoints/'\n",
        "checkpoint = tf.train.Checkpoint(agent=agent)\n",
        "\n",
        "# Guardar el modelo cada X iteraciones\n",
        "save_interval = 40000  # Guardar cada 5000 iteraciones\n",
        "\n",
        "for iteration in range(num_iterations):\n",
        "    # Recolectar experiencia\n",
        "    for _ in range(collect_steps_per_iteration):\n",
        "        collect_step(train_env, agent.collect_policy, replay_buffer)  # Se llama a la función para recolectar experiencias\n",
        "\n",
        "    # Muestra una experiencia del buffer y entrena al agente\n",
        "    experience, _ = next(iterator)  # Se extrae un lote de experiencia del buffer\n",
        "    train_loss = agent.train(experience).loss  # Se entrena al agente y se obtiene la pérdida\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if iteration % log_interval == 0:  # para cada intervalo\n",
        "        print(f'Iteración: {iteration}, Pérdida: {train_loss}')  # Se imprime la iteración y la pérdida\n",
        "\n",
        "#     # Capturar el estado del entorno (evaluated_image) después de recolectar experiencia\n",
        "#     if iteration % eval_interval == 0:\n",
        "#         avg_reward = evaluate_agent(agent, eval_env)\n",
        "#         print(f'Evaluación en iteración {iteration}: Recompensa media = {avg_reward}')\n",
        "\n",
        "    if iteration % save_interval == 0:\n",
        "        checkpoint.save(file_prefix=checkpoint_dir)\n",
        "        print(f'Modelo guardado en la iteración {iteration}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-01T18:39:22.822090Z",
          "iopub.execute_input": "2024-10-01T18:39:22.822477Z"
        },
        "trusted": true,
        "id": "WidT66ubOXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el rendimiento del agente\n",
        "#log_interval = 500  # Se define el intervalo para registrar información\n",
        "#num_eval_episodes = 7  # Se establece el número de episodios de evaluación\n",
        "\n",
        "#for episode in range(num_eval_episodes):  # Se itera sobre el número de episodios de evaluación\n",
        "   # time_step = eval_env.reset()  # Se reinicia el entorno de evaluación\n",
        "    #episode_reward = 0  # Se inicializa la recompensa total del episodio\n",
        "    #i = 0  # Se inicializa un contador de iteraciones\n",
        "\n",
        "   # while not time_step.is_last():  # Se ejecuta mientras no se alcance el final del episodio\n",
        "      #  action_step = agent.policy.action(time_step)  # Se selecciona una acción según la política del agente\n",
        "       # time_step = eval_env.step(action_step.action)  # Se avanza en el entorno con la acción seleccionada\n",
        "       # episode_reward += time_step.reward  # Se acumula la recompensa del paso actual\n",
        "       # i += 1  # Se incrementa el contador de iteraciones\n",
        "\n",
        "       # if i % 2000 == 0:  # Se verifica si se alcanza el intervalo de registro\n",
        "         #   print(f'Episode {episode + 1}: Ite.: {i} :episode_reward : {episode_reward.numpy()[0]}')  # Se imprime la recompensa acumulada\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "buqsvyo2OXlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint.save(file_prefix=checkpoint_dir)\n",
        "print(f'Modelo guardado en la iteración {iteration}')"
      ],
      "metadata": {
        "trusted": true,
        "id": "5NabpeT_OXlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar una pantalla virtual para renderizar entornos de OpenAI Gym\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()  # Se inicia la pantalla virtual\n",
        "\n",
        "# Funciones de visualización proporcionadas\n",
        "def update_scene(num, frames, patch):  # Se define la función para actualizar la escena\n",
        "    patch.set_data(frames[num])  # Se actualizan los datos del frame actual\n",
        "    return patch  # Se devuelve el parche actualizado\n",
        "\n",
        "def plot_animation(frames, repeat=False, interval=40):  # Se define la función para crear la animación\n",
        "    fig = plt.figure()  # Se crea una nueva figura\n",
        "    patch = plt.imshow(frames[0])  # Se muestra el primer frame\n",
        "    plt.axis('off')  # Se ocultan los ejes\n",
        "    anim = animation.FuncAnimation(  # Se crea la animación\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()  # Se cierra la figura para no mostrarla dos veces\n",
        "    return anim  # Se devuelve la animación creada\n",
        "\n",
        "# Recoger frames para la visualización\n",
        "def run_and_visualize(agent, env):\n",
        "    frames = []\n",
        "    time_step = env.reset()\n",
        "    policy_state = agent.policy.get_initial_state(env.batch_size)\n",
        "    #j = 0\n",
        "    while not time_step.is_last():\n",
        "    #for j in range(30000):\n",
        "        action_step = agent.policy.action(time_step, policy_state)\n",
        "        policy_state = action_step.state\n",
        "        time_step = env.step(action_step.action)\n",
        "        frame = np.squeeze(env.render())\n",
        "        frames.append(frame)\n",
        "#         if j % 500 == 0:\n",
        "        #j += 1\n",
        "#             print(f\"Step: {len(frames)}, Action: {action_step.action}, Frame shape: {frame.shape}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Total frames collected: {len(frames)}\")\n",
        "    return frames\n",
        "\n",
        "# Ejecutar el agente y recoger marcos\n",
        "frames1 = run_and_visualize(agent, eval_env)  # Se ejecuta la función para recoger marcos\n",
        "\n",
        "# Crear y mostrar la animación\n",
        "anim1 = plot_animation(frames1)  # Se crea la animación a partir de los marcos\n",
        "\n",
        "# Mostrar la animación en Jupyter Notebook\n",
        "from IPython.display import HTML  # Se importa la librería necesaria\n",
        "HTML(anim1.to_jshtml())  # Se convierte la animación a formato HTML y se muestra\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "BfBDcqggOXlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames = 0"
      ],
      "metadata": {
        "trusted": true,
        "id": "60Nc6PMQOXlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
      ],
      "metadata": {
        "trusted": true,
        "id": "V3lrTn6nOXlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mario aleatorio****"
      ],
      "metadata": {
        "id": "3uFVvh4lOXlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_env1 = tf_py_environment.TFPyEnvironment(train_py_env1)\n",
        "eval_env1 = tf_py_environment.TFPyEnvironment(eval_py_env1)\n",
        "\n",
        "# Configurar una pantalla virtual para renderizar entornos de OpenAI Gym\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()  # Se inicia la pantalla virtual\n",
        "\n",
        "# Funciones de visualización proporcionadas\n",
        "def update_scene(num, frames, patch):  # Se define la función para actualizar la escena\n",
        "    patch.set_data(frames[num])  # Se actualizan los datos del frame actual\n",
        "    return patch  # Se devuelve el parche actualizado\n",
        "\n",
        "def plot_animation1(frames, repeat=False, interval=40):  # Se define la función para crear la animación\n",
        "    fig = plt.figure()  # Se crea una nueva figura\n",
        "    patch = plt.imshow(frames[0])  # Se muestra el primer frame\n",
        "    plt.axis('off')  # Se ocultan los ejes\n",
        "    anim = animation.FuncAnimation(  # Se crea la animación\n",
        "        fig, update_scene, fargs=(frames, patch),\n",
        "        frames=len(frames), repeat=repeat, interval=interval)\n",
        "    plt.close()  # Se cierra la figura para no mostrarla dos veces\n",
        "    return anim  # Se devuelve la animación creada\n",
        "def random_policy():\n",
        "    return np.random.choice(len(SIMPLE_MOVEMENT))\n",
        "# Recoger frames para la visualización\n",
        "def run_and_visualize1(env):\n",
        "    frames = []\n",
        "    time_step = env.reset()\n",
        "    #policy_state = agent.policy.get_initial_state(env.batch_size)\n",
        "\n",
        "    for i in range(8000):\n",
        "        action_step = random_policy()\n",
        "        #policy_state = action_step.state\n",
        "        time_step = env.step(action_step)\n",
        "\n",
        "        # Obtener el frame y asegurarse de que es un array numpy\n",
        "        frame = np.squeeze(env.render())\n",
        "\n",
        "        frames.append(frame)\n",
        "        i += 1\n",
        "\n",
        "    print(f\"Total frames collected: {len(frames)}\")\n",
        "    return frames\n",
        "\n",
        "\n",
        "# Ejecutar el agente y recoger marcos\n",
        "frames_random = run_and_visualize1(eval_env1)  # Se ejecuta la función para recoger marcos\n",
        "\n",
        "# Crear y mostrar la animación\n",
        "anim = plot_animation1(frames_random)  # Se crea la animación a partir de los marcos\n",
        "\n",
        "# Mostrar la animación en Jupyter Notebook\n",
        "from IPython.display import HTML  # Se importa la librería necesaria\n",
        "HTML(anim.to_jshtml())  # Se convierte la animación a formato HTML y se muestra\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "3fTnKosqOXlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBoszKebOXlT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}